"""SDL (Singularity Data Lake) command-line tool interface.

This module provides tools for interacting with Scalyr's SDL service through the MCP server,
including functionality for executing queries and handling power queries.
"""

# TEMPORARY: Using Optional[T] instead of T | None for FastMCP compatibility
# FastMCP's current OpenAI function schema generation requires Optional[T] syntax.
# TODO: Migrate to PEP 604 unions (T | None) once FastMCP supports it.

import logging
import os
from datetime import datetime, timedelta, timezone
from textwrap import dedent
from typing import Final, Literal

from dateutil.relativedelta import relativedelta

from purple_mcp.config import get_settings
from purple_mcp.libs.sdl import (
    SDL_API_PATH,
    SDLHandlerError,
    SDLPowerQueryHandler,
    SDLPQFrequency,
    SDLPQResultType,
    SDLQueryPriority,
    create_sdl_settings,
)

# TEMPORARY: Using Optional[T] instead of T | None in this file for FastMCP compatibility.
# FastMCP's current OpenAI function schema generation requires explicit Optional types to properly
# handle optional parameters in the JSON schema format.
# TODO: Migrate to PEP 604 unions (T | None) once FastMCP supports modern union syntax.

logger = logging.getLogger(__name__)


POWERQUERY_DESCRIPTION: Final[str] = dedent("""
    Execute advanced PowerQuery analytics on data in SentinelOne's Singularity Data Lake for complex threat hunting and data analysis.

    PowerQuery is SentinelOne's high-performance query language for searching, transforming, and aggregating telemetry and log data in the Scalyr and Singularity XDR platforms. It uses a pipeline-based syntax for filtering, grouping, computing, and summarizing large-scale unstructured data.  SentinelOne PowerQuery is not the same as Microsoft PowerQuery.  It also looks somewhat like Splunk SPL but is not the same language.

    IMPORTANT: You should ALWAYS use the purple_ai() tool to generate PowerQueries for this tool based on natural language.  It is very unlikely you know how to write PowerQueries yourself.

    If a user gives you a specific PowerQuery that wasn't generated by Purple AI, run it EXACTLY as sent.  DO NOT modify the user's input, pass it directly to this tool.

    Time Range Guidelines:
    - DEFAULT to a 24-hour search period unless the user specifies a different time range
    - Use get_timestamp_range(hours=24) to get the last 24 hours
    - Queries with longer time ranges may take longer to execute or timeout
    - The query timeout is 5 minutes; if exceeded, reduce the time range or simplify the query

    Understanding Results:
    - Empty results (no data returned) are NORMAL and ACCEPTABLE. This means no records match your query criteria.
    - Empty results could indicate:
      * No matching data exists in the specified time range
      * The query conditions are too restrictive
      * The searched activity or behavior simply hasn't occurred
    - DO NOT repeatedly rephrase or retry queries that legitimately return no results
    - If you expect results but get none, consider adjusting the time range or query criteria rather than retrying the exact same query
    """).strip()

GET_TIMESTAMP_RANGE_DESCRIPTION: Final[str] = dedent("""
    Generate time range timestamps for PowerQuery analytics in SentinelOne's Singularity Data Lake.
    """).strip()


def _iso_to_nanoseconds(iso_string: str) -> int:
    """Convert ISO 8601 string to nanoseconds since Unix epoch.

    Args:
        iso_string: ISO 8601 datetime string with timezone (e.g., "2024-01-15T10:30:00Z")

    Returns:
        Nanoseconds since Unix epoch

    Raises:
        ValueError: If the ISO string is invalid or lacks timezone information

    Examples:
        >>> _iso_to_nanoseconds("2024-01-15T10:30:00Z")
        1705315800000000000
        >>> _iso_to_nanoseconds("2024-01-15T10:30:00+05:00")
        1705297800000000000
    """
    from dateutil import parser

    try:
        # Parse the ISO 8601 string using dateutil for robust parsing
        dt = parser.parse(iso_string)

        # Check if the parsed datetime has timezone information
        if dt.tzinfo is None:
            raise ValueError(
                f"Timestamp must include explicit timezone information (e.g., 'Z' or '±HH:MM'). "
                f"Received naive timestamp: {iso_string}"
            )

        # Convert to nanoseconds since Unix epoch
        return int(dt.timestamp() * 1_000_000_000)
    except ValueError as e:
        # Check if this is our explicit timezone validation error
        if "explicit timezone information" in str(e):
            # Re-raise our timezone validation error as-is
            raise
        # Otherwise, wrap dateutil parser errors with a consistent message
        raise ValueError(f"Invalid ISO 8601 datetime string: {iso_string}") from e
    except (TypeError, OverflowError) as e:
        raise ValueError(f"Invalid ISO 8601 datetime string: {iso_string}") from e


def _nanoseconds_to_datetime(timestamp_ns: int) -> datetime:
    """Convert nanoseconds since epoch to datetime object.

    Args:
        timestamp_ns: Timestamp in nanoseconds since epoch

    Returns:
        datetime object in UTC timezone
    """
    # Convert nanoseconds to seconds
    timestamp_seconds = timestamp_ns / 1_000_000_000
    return datetime.fromtimestamp(timestamp_seconds, tz=timezone.utc)


async def powerquery(  # noqa: C901
    query: str,
    start_datetime: str,
    end_datetime: str,
) -> str:
    """Run a SentinelOne PowerQuery in AISIEM and return the results.

    Args:
        query: The PowerQuery string to execute
        start_datetime: Start time in ISO 8601 format. ISO 8601 is the international standard
            for datetime representation: YYYY-MM-DDTHH:MM:SS with timezone offset.
            MUST include timezone offset (Z for UTC or ±HH:MM for local time).
            Examples: "2024-01-15T10:30:00Z" (UTC/Zulu time),
            "2024-01-15T10:30:00+05:00" (UTC+5 hours, e.g., Asia/Karachi),
            "2024-01-15T10:30:00-08:00" (UTC-8 hours, e.g., US Pacific),
            "2024-01-15T10:30:00.123456+02:00" (with microseconds, UTC+2)
        end_datetime: End time in ISO 8601 format. Same format requirements as start_datetime.
            Must be later than start_datetime. Examples: "2024-01-15T11:30:00Z" (UTC),
            "2024-01-15T11:30:00+05:00" (UTC+5, same timezone as start),
            "2024-01-15T11:30:00-08:00" (UTC-8, same timezone as start)

    Returns:
        String representation of the query results
    """
    # Validate settings are available
    try:
        settings = get_settings()
    except Exception as exc:
        from purple_mcp.config import ENV_PREFIX

        raise ValueError(
            f"Application configuration is not available. "
            f"Please ensure required environment variables are set: "
            f"{ENV_PREFIX}CONSOLE_TOKEN, {ENV_PREFIX}CONSOLE_BASE_URL. Error: {exc}"
        ) from exc

    # Log non-sensitive metadata at INFO level
    logger.info(
        "Running PowerQuery",
        extra={
            "token_configured": bool(settings.sdl_api_token),
        },
    )

    # Log detailed query information at DEBUG level for troubleshooting
    # Only log full query if unsafe debugging is explicitly enabled
    if os.environ.get("PURPLEMCP_DEBUG_UNSAFE_LOGGING") == "1":
        logger.debug(
            "PowerQuery details",
            extra={
                "query": query,
                "start_datetime": start_datetime,
                "end_datetime": end_datetime,
            },
        )
    else:
        logger.debug(
            "PowerQuery details",
            extra={
                "query_length": len(query),
                "has_query": bool(query),
                "has_start_datetime": bool(start_datetime),
                "has_end_datetime": bool(end_datetime),
            },
        )

    # Convert ISO strings to nanoseconds and then to datetime objects
    start_ns = _iso_to_nanoseconds(start_datetime)
    end_ns = _iso_to_nanoseconds(end_datetime)
    start_time = _nanoseconds_to_datetime(start_ns)
    end_time = _nanoseconds_to_datetime(end_ns)

    # Validate that end_datetime is later than start_datetime
    if end_time <= start_time:
        raise ValueError(
            f"end_datetime must be later than start_datetime. "
            f"Got start_datetime={start_datetime}, end_datetime={end_datetime}"
        )

    # Create SDL configuration
    sdl_settings = create_sdl_settings(
        auth_token=settings.sdl_api_token,
        # Use the actual console base URL with SDL endpoint
        base_url=settings.sentinelone_console_base_url + SDL_API_PATH,
        # Increase timeout for potentially long-running queries
        default_poll_timeout_ms=300_000,  # 5 minutes
        http_timeout=60,  # 1 minute for HTTP requests
        # Pass environment from application settings
        environment=settings.environment,
    )

    # Ensure auth token is available
    if not sdl_settings.auth_token:
        raise ValueError("PowerQuery logs token is not configured or is empty")

    # Create the SDL PowerQuery handler
    handler = SDLPowerQueryHandler(
        auth_token=sdl_settings.auth_token,
        base_url=sdl_settings.base_url,
        settings=sdl_settings,
    )

    try:
        # Submit the PowerQuery
        await handler.submit_powerquery(
            start_time=start_time,
            end_time=end_time,
            query=query,
            result_type=SDLPQResultType.TABLE,
            frequency=SDLPQFrequency.LOW,
            query_priority=SDLQueryPriority.LOW,
        )

        # Poll until the query completes
        results = await handler.poll_until_complete()

        # Check if results are partial
        if handler.is_result_partial():
            logger.warning("PowerQuery returned partial results")

        # Format and return results
        if not results or not results.values:
            logger.info("PowerQuery returned no results")
            return ""

        # Create a formatted string representation of the results
        result_lines = []

        # Add metadata
        result_lines.append(f"Match Count: {results.match_count}")
        result_lines.append(f"Columns: {len(results.columns)}")
        result_lines.append(f"Rows: {len(results.values)}")

        if results.warnings:
            result_lines.append(f"Warnings: {results.warnings}")

        # Add column headers
        if results.columns:
            column_names = [col.name for col in results.columns]
            result_lines.append(f"Column Names: {', '.join(column_names)}")

        # Add sample of results (first 100 rows to avoid overwhelming output)
        if results.values:
            result_lines.append("\nResults:")
            max_rows = min(100, len(results.values))
            for i in range(max_rows):
                row_data = results.values[i]
                result_lines.append(f"Row {i + 1}: {row_data}")

            if len(results.values) > max_rows:
                result_lines.append(f"... and {len(results.values) - max_rows} more rows")

        return "\n".join(result_lines)

    except SDLHandlerError:
        logger.exception("SDL handler error occurred during PowerQuery execution")
        raise
    except Exception:
        logger.exception("Unexpected error executing PowerQuery")
        raise
    finally:
        # Ensure cleanup happens on all paths (success, error, or exception)
        # Delete the query if it was submitted and we have the necessary IDs
        if (
            handler.query_submitted
            and handler.query_id
            and not handler.sdl_query_client.is_closed()
        ):
            try:
                if not handler.is_query_completed():
                    # Query was submitted but didn't complete - still try to delete it
                    await handler.delete_query()
            except Exception:
                logger.exception("Failed to delete query during cleanup")

        # Always close the HTTP client to prevent resource leaks
        if not handler.sdl_query_client.is_closed():
            try:
                await handler.sdl_query_client.close()
            except Exception:
                logger.exception("Failed to close SDL query client during cleanup")


def get_timestamp_range(
    reference_time: str | None = None,
    years: int = 0,
    months: int = 0,
    weeks: int = 0,
    days: int = 0,
    hours: int = 0,
    minutes: int = 0,
    seconds: int = 0,
    direction: Literal["past", "future"] = "past",
) -> dict[str, str]:
    """Get both current timestamp and offset timestamp for time range queries.

    This tool returns both the current time and a calculated offset time, designed
    specifically for PowerQuery time range queries. Returns both timestamps in a
    single call to ensure consistency.

    Args:
        reference_time: Reference ISO 8601 timestamp to calculate from. If None, uses current time.
            Examples: "2024-01-15T14:30:00Z", "2024-01-15T14:30:00+05:00"
        years: Number of years to offset (default: 0)
        months: Number of months to offset (default: 0)
        weeks: Number of weeks to offset (default: 0)
        days: Number of days to offset (default: 0)
        hours: Number of hours to offset (default: 0)
        minutes: Number of minutes to offset (default: 0)
        seconds: Number of seconds to offset (default: 0)
        direction: Whether to go into the "past" (default) or "future"

    Returns:
        Dictionary containing both timestamps:
        - "current_time": Current timestamp (or reference_time if provided)
        - "offset_time": Calculated offset timestamp

    Examples:
        Common PowerQuery time range calculations:
        - "Last 24 hours": get_timestamp_range(hours=24)
        - "Last week": get_timestamp_range(weeks=1)
        - "Last month": get_timestamp_range(months=1)
        - "Last 6 hours": get_timestamp_range(hours=6)
        - "Yesterday": get_timestamp_range(days=1)
        - "Last 30 days": get_timestamp_range(days=30)

    Usage Pattern:
        result = get_timestamp_range(hours=24)
        await powerquery(
            "your query here",
            start_datetime=result["offset_time"],  # 24 hours ago
            end_datetime=result["current_time"]    # now
        )
    """
    # Validate direction parameter
    if direction not in ("past", "future"):
        raise ValueError(f"Invalid direction '{direction}'. Must be 'past' or 'future'.")

    # Parse reference timestamp or use current time
    if reference_time is None:
        base_dt = datetime.now(timezone.utc)
        current_time = base_dt.isoformat().replace("+00:00", "Z")
    else:
        # Convert ISO string to datetime using our existing utility
        base_ns = _iso_to_nanoseconds(reference_time)
        base_dt = datetime.fromtimestamp(base_ns / 1_000_000_000, tz=timezone.utc)
        current_time = reference_time

    # Calculate the delta using both timedelta and relativedelta
    # relativedelta is needed for months and years
    if years or months:
        # Use relativedelta for months/years
        relative_delta = relativedelta(
            years=years,
            months=months,
            weeks=weeks,
            days=days,
            hours=hours,
            minutes=minutes,
            seconds=seconds,
        )

        result_dt = base_dt - relative_delta if direction == "past" else base_dt + relative_delta
    else:
        # Use timedelta for smaller units (more efficient)
        time_delta = timedelta(
            weeks=weeks, days=days, hours=hours, minutes=minutes, seconds=seconds
        )

        result_dt = base_dt - time_delta if direction == "past" else base_dt + time_delta

    # Return both timestamps
    offset_time = result_dt.isoformat().replace("+00:00", "Z")

    return {"current_time": current_time, "offset_time": offset_time}
